{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 21:52:15.262264: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-26 21:52:15.426676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-26 21:52:15.487398: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-26 21:52:15.502760: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-26 21:52:15.609868: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-26 21:52:16.819563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Close/Last', 'Volume', 'Open', 'High', 'Low'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2517, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = r\"/teamspace/studios/this_studio/nvidiaStock.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.columns)\n",
    "\n",
    "df['Close/Last'] = df['Close/Last'].str.replace('$', '', regex = False)\n",
    "df['Close/Last'] = pd.to_numeric(df['Close/Last'])\n",
    "\n",
    "df['Open'] = df['Open'].str.replace('$', '', regex = False)\n",
    "df['Open'] = pd.to_numeric(df['Open'])\n",
    "\n",
    "df['High'] = df['High'].str.replace('$', '', regex = False)\n",
    "df['High'] = pd.to_numeric(df['High'])\n",
    "\n",
    "df['Low'] = df['Low'].str.replace('$', '', regex = False)\n",
    "df['Low'] = pd.to_numeric(df['Low'])\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\n",
    "df['Date'] = pd.to_numeric(df['Date'])\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(random_state=42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns = ['High'])\n",
    "y = df['High']\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "print(type(X[0][2]))\n",
    "print(type(X[0][1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 15.510524   3.854833  18.060027  13.613093   0.569999  13.481925\n",
      "  13.784584   2.574774   6.558752   5.757925  28.52061   11.411277\n",
      " 124.044025  16.657358  19.105097  29.06278    4.286171  22.858768\n",
      "   4.423233   5.287641   9.497601   4.187872   1.35527    4.116369\n",
      "  31.32405   14.196142  39.52431   25.91412    5.284493  10.678518\n",
      "  12.361065   0.517665 130.5076     0.760544   6.161175  10.461893\n",
      "  22.909215   4.727749  12.722393   5.422187  27.263888  17.957442\n",
      "   0.683748   6.337668   5.051132   0.488899   6.951581  46.08654\n",
      "  13.26775    6.187689   0.570445   5.996191   4.463396   0.595994\n",
      "   0.83089   19.710775  13.603265   0.802444   4.448636  17.485078\n",
      "   0.441416   5.299975   0.503014  13.580713  47.852429   3.33467\n",
      "   0.812883  22.1859     6.141623   6.221689   6.235644   6.389309\n",
      "   1.173578  47.32786    5.667098   2.629725   0.464259  12.851801\n",
      "   5.38225   71.69396    6.869265   5.469329  25.68579    0.50462\n",
      "   7.271655  17.045322  18.767049  32.51546   13.559241   2.490529\n",
      "   0.717977   0.808371   1.670604   0.781488   6.183447   0.515451\n",
      "   7.050491  50.009175   0.56283   23.370844   2.987015   1.530594\n",
      "  13.221727   5.461925   0.505326  13.446911  27.140838  53.499429\n",
      "  25.414775   6.595463  90.11057    0.698357   3.762601  17.092631\n",
      "  22.820062   6.350424   0.487191   4.251374   0.550862   5.854939\n",
      "   1.337196   6.056773   3.801718  47.974269   3.913747   2.357924\n",
      "   4.224657  32.73465   42.21376    0.915294  95.20935    5.710477\n",
      "   4.101857  15.262735  39.31817    4.15315    5.216311  30.56789\n",
      "  48.53723    0.520796   9.394521   0.500433   5.190143   6.925215\n",
      "   3.356564  29.42639    0.56738    1.148586  41.990665   1.811617\n",
      "  13.297741   6.844126   6.844039   7.303013  13.300487  13.449372\n",
      " 126.2382     4.242717   4.96824    0.765249   1.179739  16.186344\n",
      "  32.1041   129.8235    12.502055   0.555177   9.446005  16.968398\n",
      "  16.773928   0.515288   0.519243  31.60943   20.111568   0.559629\n",
      "   5.205409  12.737028   4.162337   4.208713   6.279453  22.811345\n",
      "  26.793939   1.575297   2.729779   3.884485  16.001453  45.06678\n",
      "  21.899025  20.862949   4.816988   0.666826   0.720083   0.594417\n",
      " 109.0308    11.446495  28.619028   5.91635   19.658479   5.459906\n",
      "  13.471008   6.953926   1.776049   0.838593   9.09844    1.638315\n",
      "  14.239068   5.301543   0.557065   6.480496   0.559424  28.536425\n",
      "  21.855275  61.06852    8.893984   0.651092   5.259596   1.666084\n",
      "  25.23513    6.347079   4.052873   7.076798  13.561394   0.548749\n",
      "   6.147674  13.191085  22.892346   3.803948   0.500969  17.88319\n",
      "  25.14872    2.739216   4.23385    4.313862  15.33639    2.671638\n",
      "  12.917601   3.689497  91.49134    2.680702  26.907007  11.770972\n",
      "   2.4785     5.47332    0.719589  13.795343  27.434949   6.080221\n",
      " 114.56222    5.5604    20.982836  15.567523   5.715055   4.007975\n",
      "  13.181865   6.315865   0.48776   12.787453   7.100968  14.674097\n",
      "   1.18212   32.04146   31.71915    0.534146   0.512041   0.53724\n",
      "   5.473431  13.188966   5.33125    6.834393   3.386949   2.378735\n",
      "   4.397033   5.4478    13.388674  13.184421   0.886751  19.822531\n",
      "  11.579984   1.58513    0.542711   3.431636   0.559346  23.577859\n",
      "  12.785715   4.556405   9.134881   6.554036  19.809446  19.769114\n",
      "  91.67432    7.605782  72.86492   13.357669   1.108602 107.226191\n",
      "   0.531259   0.899526   0.839507   2.720113  41.92205  124.520705\n",
      "   0.492745   2.4886     3.910012   2.684667  12.504093   4.91677\n",
      "   2.920222   0.457483  13.827614   4.154818  14.245503   6.99532\n",
      "   0.521646   6.036272  28.725338   6.222129   0.539685   0.507998\n",
      "  47.035586 124.2678     0.653497  13.707966   4.294835   3.941374\n",
      "  19.53047   43.40033   90.4276    13.474812   3.801245   1.635163\n",
      "   6.037399  13.914638   0.508719   7.559988  13.638319  21.064974\n",
      "   4.036713  20.503879   0.826689  90.46127    0.487046  47.26421\n",
      "   0.646645   6.273106   3.463391   4.297829   5.695033   8.963237\n",
      "  80.49045    0.570922   2.447788 107.67235   17.40991    0.484919\n",
      "   0.590104   0.705209   6.200274   2.668444   0.502427   0.443308\n",
      "   2.486119   0.830978   5.300372   3.46767   20.03666    6.036527\n",
      "   3.781603   1.571991   0.4948    14.185269  49.932313   1.742064\n",
      "   4.697113   9.674545   0.51523    2.645      0.922608   0.764285\n",
      "   3.606224   0.570418  22.710992   0.458594  22.107968   2.833087\n",
      "   1.197759   4.151212   5.665293  49.92856  128.1889    13.068375\n",
      "   4.723153   6.953879   8.053533  62.68786   13.600055   0.49594\n",
      "  13.49295    2.500029  15.996448   4.536727 125.1668    47.84269\n",
      "   5.192916  13.206057   5.869013   4.810244  13.09458   70.00288\n",
      "  10.434895   0.575956   3.698858  16.614956   3.618323   0.895676\n",
      "   2.701474   0.511668  13.657196  32.44613   14.680062  13.810635\n",
      "  15.932454   7.506805  24.527059   2.643742  13.394403   0.701305\n",
      "  79.58642    3.809605  13.611416  47.854067  30.05308    0.503232\n",
      "  20.716817   1.084332  45.423935   2.584707   9.403093 110.575275\n",
      "   3.885833   2.641406  20.450118   0.530663   3.999698  17.585358\n",
      "  22.367855  14.963317  49.332704   0.702616  12.651969   4.064969\n",
      "  24.9724     0.535954   0.521337   6.148454  58.939177  91.66612\n",
      "   4.933459   5.021956   2.360083   0.558534 130.3494    22.073374\n",
      "   2.353508  89.97223   17.305368  16.940643  13.941177   4.414189\n",
      "   0.924174   4.132072  22.980278   2.838498  43.849955   1.181976\n",
      "   5.434821   2.752474   7.09467    5.543313  18.825514   4.810972\n",
      "   6.43612    0.506774  14.300057  15.582532   0.532638   5.706238\n",
      "   5.752022   0.529895   0.488976   1.408881  13.91622    6.30292\n",
      " 122.95542    9.022711   3.370616   4.530907   2.730371   1.522788\n",
      "  15.03951    4.119363  49.000334  49.980216   0.53549   22.39623 ]\n",
      "[ 15.3625   3.8238  18.255   13.6058   0.5843  13.405   13.6735   2.5747\n",
      "   6.5862   5.8875  28.755   11.4083 125.587   16.487   19.619   28.969\n",
      "   4.314   23.033    4.445    5.5215   9.5313   4.182    1.3668   4.0968\n",
      "  31.1     14.2435  39.711   26.5432   5.4183  10.6553  12.55     0.5175\n",
      " 130.77     0.7473   6.197   10.547   23.02     4.6843  12.736    5.425\n",
      "  27.33    17.856    0.6946   6.38     5.067    0.4885   6.9725  46.053\n",
      "  13.3253   6.213    0.571    5.9722   4.4755   0.5938   0.8315  20.037\n",
      "  13.5998   0.8019   4.4068  17.406    0.4412   5.3338   0.5048  13.6118\n",
      "  48.11     3.24     0.8303  21.931    6.1408   6.1898   6.2228   6.3243\n",
      "   1.1685  47.116    5.6388   2.6598   0.4648  13.175    5.355   70.794\n",
      "   6.837    5.5493  26.569    0.5053   7.3015  16.998   19.245   32.71\n",
      "  14.031    2.5158   0.717    0.8085   1.6695   0.7793   6.17     0.513\n",
      "   7.1042  49.96     0.5623  23.355    3.0455   1.5325  13.3945   5.4668\n",
      "   0.5035  13.464   27.152   54.325   26.152    6.6113  89.724    0.6957\n",
      "   3.7423  17.14    22.788    6.5779   0.4858   4.2618   0.5525   5.8805\n",
      "   1.33     6.0563   3.7925  47.818    3.8925   2.3433   4.245   33.2893\n",
      "  42.018    0.912   96.766    5.7348   4.2138  15.177   38.864    4.1478\n",
      "   5.1953  31.0865  48.472    0.525    9.5      0.4995   5.1345   6.91\n",
      "   3.2495  30.5      0.5735   1.1495  41.6      1.8025  13.4428   6.73\n",
      "   6.7355   7.296   13.2327  13.421  124.69     4.2498   4.9993   0.768\n",
      "   1.1695  16.028   32.129  129.04    12.5225   0.5553   9.5564  16.915\n",
      "  17.111    0.5125   0.518   31.828   20.0988   0.557    5.1623  12.611\n",
      "   4.1678   4.2358   6.235   22.711   26.638    1.5807   2.7459   3.8462\n",
      "  16.0576  45.175   22.373   21.063    4.7913   0.6625   0.7168   0.592\n",
      " 106.6     11.358   29.0584   5.9     20.1285   5.425   13.6928   6.9775\n",
      "   1.793    0.8373   9.1873   1.6323  14.4738   5.3638   0.5585   6.5455\n",
      "   0.5588  28.922   21.949   60.331    8.8373   0.654    5.247    1.6695\n",
      "  25.3      6.3543   4.0476   7.133   13.4349   0.5435   6.1605  13.0518\n",
      "  22.695    3.8345   0.4997  17.728   25.182    2.7538   4.2518   4.3362\n",
      "  15.319    2.653   12.85     3.6573  90.998    2.7     26.513   12.111\n",
      "   2.4675   5.434    0.7183  13.838   27.469    6.0453 112.717    5.62\n",
      "  21.017   15.773    5.903    4.022   13.233    6.2995   0.4875  12.823\n",
      "   7.1105  14.547    1.1775  31.288   31.33     0.5425   0.5125   0.535\n",
      "   5.55    13.3348   5.3712   6.756    3.3095   2.3685   4.3985   5.4292\n",
      "  13.6525  13.315    0.8856  19.825   11.467    1.5813   0.5463   3.4347\n",
      "   0.5625  24.057   12.648    4.546    9.2613   6.591   19.77    19.6337\n",
      "  91.651    7.55    73.45    13.29     1.0965 108.72     0.5298   0.8935\n",
      "   0.8435   2.7218  41.766  126.5      0.4967   2.481    3.8975   2.6713\n",
      "  12.496    4.875    2.9332   0.4549  13.7373   4.1638  14.262    7.0429\n",
      "   0.521    6.0278  31.365    6.1975   0.5425   0.5171  47.2    124.07\n",
      "   0.6445  13.6448   4.3093   3.9333  19.42    43.903   90.374   13.575\n",
      "   3.8645   1.615    6.0123  14.21     0.5064   7.605   13.8845  21.066\n",
      "   4.0085  20.4      0.8265  91.96     0.488   46.98     0.6638   6.2835\n",
      "   3.545    4.3312   5.72     8.9165  80.646    0.5693   2.4303 105.5\n",
      "  17.32     0.4832   0.5988   0.7027   6.2338   2.61     0.5043   0.4457\n",
      "   2.4693   0.832    5.3425   3.4947  20.222    6.0098   3.767    1.5653\n",
      "   0.4938  14.2388  49.997    1.7395   4.71     9.5757   0.5158   2.641\n",
      "   0.9158   0.764    3.6298   0.571   22.593    0.4588  21.74     2.8495\n",
      "   1.192    4.2665   5.6248  49.744  126.41    13.388    4.6682   7.02\n",
      "   8.0785  62.719   13.6495   0.4948  13.4435   2.4883  15.995    4.5355\n",
      " 124.84    48.187    5.1058  13.2137   5.883    4.7983  13.0463  69.754\n",
      "  10.4418   0.5787   3.6983  16.627    3.6345   0.8955   2.7873   0.5128\n",
      "  13.961   32.855   14.5713  13.8285  16.002    7.5068  24.326    2.6348\n",
      "  13.3372   0.703   82.394    3.747   13.5933  48.23    30.294    0.5025\n",
      "  21.192    1.0963  45.789    2.6255   9.2825 111.99     3.8958   2.6175\n",
      "  20.508    0.5293   4.0928  17.538   22.433   14.996   49.5247   0.698\n",
      "  12.6325   4.0145  25.588    0.534    0.5195   6.157   59.5     90.41\n",
      "   4.9438   4.9688   2.361    0.5555 129.35    22.0776   2.3073  89.281\n",
      "  17.687   18.092   13.925    4.4375   0.9195   4.1575  23.13     2.795\n",
      "  43.99     1.1858   5.4623   2.7473   7.1968   5.5394  18.846    4.775\n",
      "   6.3908   0.507   14.6915  15.473    0.5318   5.6568   5.7244   0.5287\n",
      "   0.4875   1.3953  14.3748   6.2638 121.6917   9.093    3.3285   4.516\n",
      "   2.75     1.5575  14.9708   4.109   49.116   50.548    0.5365  22.149 ]\n",
      "0.0\n",
      "0.14225698467749792\n",
      "0.999771721962806\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "print(prediction)\n",
    "print(y_test)\n",
    "\n",
    "print(np.mean(prediction==y_test))\n",
    "print(mean_squared_error(prediction, y_test))\n",
    "print(r2_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nvidiaPrediction.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model, \"nvidiaPrediction.joblib\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
